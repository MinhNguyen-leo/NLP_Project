{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d752adbd",
   "metadata": {},
   "source": [
    "# Seq2Seq Translation with LSTM  \n",
    "### English to French Machine Translation\n",
    "\n",
    "**Bao gồm:**\n",
    "- Chuẩn bị dữ liệu (raw → processed)\n",
    "- Tạo từ vựng (src + tgt)\n",
    "- Dataset + DataLoader\n",
    "- Encoder–Decoder LSTM\n",
    "- Training loop (teacher forcing)\n",
    "- Inference (translate function)\n",
    "- Evaluation (BLEU score)\n",
    "- 5 ví dụ dịch + phân tích lỗi\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b517346",
   "metadata": {},
   "source": [
    "### Import thư viện "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4f66a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7f443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hanhut/miniconda3/envs/nlp/bin/python3.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deaa634",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f370523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_parallel(en_path, fr_path):\n",
    "#     en_lines = open(en_path, 'r', encoding='utf-8').read().strip().split('\\n')\n",
    "#     fr_lines = open(fr_path, 'r', encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "#     assert len(en_lines) == len(fr_lines), \"Số dòng 2 file không khớp nhau!\"\n",
    "\n",
    "#     pairs = []\n",
    "\n",
    "#     for en, fr in zip(en_lines, fr_lines):\n",
    "#         pairs.append((en.lower(), fr.lower()))\n",
    "    \n",
    "#     return pairs\n",
    "\n",
    "# train_pairs = load_parallel('./data/raw/train.en', './data/raw/train.fr')\n",
    "# val_pairs = load_parallel('./data/raw/val.en', './data/raw/val.fr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b8d36",
   "metadata": {},
   "source": [
    "# 2. Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae03c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIL_TOKENS = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "\n",
    "# def tokenize(sentence):\n",
    "#     return sentence.strip().split()\n",
    "\n",
    "# def yield_tokens(pairs):\n",
    "#     for en, fr in pairs:\n",
    "#         for token in tokenize(en):\n",
    "#             yield token\n",
    "#         for token in tokenize(fr):\n",
    "#             yield token\n",
    "\n",
    "# def build_vocab(iterator):\n",
    "#     vocab = build_vocab_from_iterator(iterator, \n",
    "#                                     min_freq=2,\n",
    "#                                     specials=SPECIL_TOKENS,\n",
    "#                                     special_first=True,\n",
    "#                                     max_tokens=10000)\n",
    "#     vocab.set_default_index(vocab['<unk>'])\n",
    "#     return vocab\n",
    "\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a1cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Cell: Chuẩn bị dữ liệu (tokenize, build vocab, dataset, dataloader) ===\n",
    "# # Yêu cầu: raw files dạng data/raw/train.en, data/raw/train.fr, valid.en/valid.fr, test.en/test.fr\n",
    "# # Sử dụng spaCy tokenizer; vocab giới hạn 10000; thêm tokens: <unk>, <pad>, <sos>, <eos>.\n",
    "# # Nếu torchtext khả dụng -> dùng build_vocab_from_iterator; nếu không -> fallback bằng Counter.\n",
    "\n",
    "# import warnings\n",
    "# from collections import Counter\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # --- spaCy tokenizer (bắt buộc đã cài en_core_web_sm và fr_core_news_sm) ---\n",
    "# import spacy\n",
    "# en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "# fr_nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# def tokenize_en(text):\n",
    "#     return [tok.text.lower() for tok in en_nlp(text)]\n",
    "\n",
    "# def tokenize_fr(text):\n",
    "#     return [tok.text.lower() for tok in fr_nlp(text)]\n",
    "\n",
    "# # --- Try to import torchtext build_vocab_from_iterator; fallback if not present ---\n",
    "# USE_TORCHTEXT = False\n",
    "# try:\n",
    "#     from torchtext.vocab import build_vocab_from_iterator\n",
    "#     USE_TORCHTEXT = True\n",
    "# except Exception as e:\n",
    "#     warnings.warn(\"torchtext unavailable or failed to load; falling back to Counter-based vocab builder.\\n\" + str(e))\n",
    "#     USE_TORCHTEXT = False\n",
    "\n",
    "# # --- Special tokens and options ---\n",
    "# PAD_TOKEN = \"<pad>\"\n",
    "# SOS_TOKEN = \"<sos>\"\n",
    "# EOS_TOKEN = \"<eos>\"\n",
    "# UNK_TOKEN = \"<unk>\"\n",
    "# SPECIALS = [UNK_TOKEN, PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "# MAX_VOCAB = 10000\n",
    "# MIN_FREQ = 1   # you can set to 2 to filter very rare tokens if desired\n",
    "\n",
    "# # --- Helpers to read parallel files (EN/FR) ---\n",
    "# def load_parallel(en_path, fr_path):\n",
    "#     with open(en_path, encoding=\"utf8\") as f:\n",
    "#         en_lines = [l.strip() for l in f if l.strip()]\n",
    "#     with open(fr_path, encoding=\"utf8\") as f:\n",
    "#         fr_lines = [l.strip() for l in f if l.strip()]\n",
    "#     assert len(en_lines) == len(fr_lines), f\"Mismatch lines: {len(en_lines)} != {len(fr_lines)}\"\n",
    "#     return list(zip(en_lines, fr_lines))\n",
    "\n",
    "# # Example usage:\n",
    "# # train_pairs = load_parallel(\"data/raw/train.en\", \"data/raw/train.fr\")\n",
    "\n",
    "# # --- Build vocab: two implementations ---\n",
    "# if USE_TORCHTEXT:\n",
    "#     # Using torchtext.build_vocab_from_iterator (preferred when available)\n",
    "#     def yield_tokens_from_pairs(pairs, lang=\"en\"):\n",
    "#         if lang == \"en\":\n",
    "#             for en, fr in pairs:\n",
    "#                 yield tokenize_en(en)\n",
    "#         else:\n",
    "#             for en, fr in pairs:\n",
    "#                 yield tokenize_fr(fr)\n",
    "\n",
    "#     def build_vocab_torchtext(pairs, lang=\"en\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ):\n",
    "#         iterator = yield_tokens_from_pairs(pairs, lang=lang)\n",
    "#         vocab = build_vocab_from_iterator(iterator,\n",
    "#                                           specials=SPECIALS,\n",
    "#                                           special_first=True,\n",
    "#                                           max_tokens=max_tokens)\n",
    "#         vocab.set_default_index(vocab[UNK_TOKEN])\n",
    "#         return vocab\n",
    "\n",
    "# else:\n",
    "#     # Fallback: use Counter and build dict\n",
    "#     def build_vocab_counter(pairs, lang=\"en\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ):\n",
    "#         counter = Counter()\n",
    "#         if lang == \"en\":\n",
    "#             for en, fr in pairs:\n",
    "#                 counter.update(tokenize_en(en))\n",
    "#         else:\n",
    "#             for en, fr in pairs:\n",
    "#                 counter.update(tokenize_fr(fr))\n",
    "#         # keep most common up to max_tokens - len(SPECIALS)\n",
    "#         most_common = [w for w, c in counter.most_common(max_tokens - len(SPECIALS)) if c >= min_freq]\n",
    "#         vocab_list = SPECIALS + most_common\n",
    "#         stoi = {w: i for i, w in enumerate(vocab_list)}\n",
    "#         # helper object to mimic torchtext vocab API minimally\n",
    "#         class VocabLike:\n",
    "#             def __init__(self, stoi, itos):\n",
    "#                 self.stoi = stoi\n",
    "#                 self.itos = itos\n",
    "#             def __len__(self):\n",
    "#                 return len(self.itos)\n",
    "#             def __getitem__(self, token):\n",
    "#                 return self.stoi.get(token, self.stoi[UNK_TOKEN])\n",
    "#             def get_itos(self):\n",
    "#                 return self.itos\n",
    "#             def get_stoi(self):\n",
    "#                 return self.stoi\n",
    "#             def set_default_index(self, idx):\n",
    "#                 pass\n",
    "#         itos = vocab_list\n",
    "#         return VocabLike(stoi, itos)\n",
    "\n",
    "# # --- Encode / numericalize helpers (works for both vocab types) ---\n",
    "# def tokens_to_ids(tokens, vocab, specials_map=None):\n",
    "#     # vocab could be torchtext.Vocab or our VocabLike\n",
    "#     ids = []\n",
    "#     if USE_TORCHTEXT:\n",
    "#         for t in tokens:\n",
    "#             ids.append(vocab[t])\n",
    "#     else:\n",
    "#         for t in tokens:\n",
    "#             ids.append(vocab.stoi.get(t, vocab.stoi[UNK_TOKEN]))\n",
    "#     return ids\n",
    "\n",
    "# def encode_sentence_en(text, vocab):\n",
    "#     toks = tokenize_en(text)\n",
    "#     ids = [ (vocab[SOS_TOKEN] if USE_TORCHTEXT else vocab.stoi[SOS_TOKEN]) ]\n",
    "#     ids += tokens_to_ids(toks, vocab)\n",
    "#     ids += [ (vocab[EOS_TOKEN] if USE_TORCHTEXT else vocab.stoi[EOS_TOKEN]) ]\n",
    "#     return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# def encode_sentence_fr(text, vocab):\n",
    "#     toks = tokenize_fr(text)\n",
    "#     ids = [ (vocab[SOS_TOKEN] if USE_TORCHTEXT else vocab.stoi[SOS_TOKEN]) ]\n",
    "#     ids += tokens_to_ids(toks, vocab)\n",
    "#     ids += [ (vocab[EOS_TOKEN] if USE_TORCHTEXT else vocab.stoi[EOS_TOKEN]) ]\n",
    "#     return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# # --- Dataset + collate_fn (pad + sort by length descending + return lengths) ---\n",
    "# class ParallelDataset(Dataset):\n",
    "#     def __init__(self, pairs, src_vocab, tgt_vocab):\n",
    "#         self.pairs = pairs\n",
    "#         self.src_vocab = src_vocab\n",
    "#         self.tgt_vocab = tgt_vocab\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.pairs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         en, fr = self.pairs[idx]\n",
    "#         src_ids = encode_sentence_en(en, self.src_vocab)\n",
    "#         tgt_ids = encode_sentence_fr(fr, self.tgt_vocab)\n",
    "#         return src_ids, tgt_ids\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     # batch: list of (src_tensor, tgt_tensor)\n",
    "#     src_list, tgt_list = zip(*batch)\n",
    "#     # sort by src length desc (required for pack_padded_sequence)\n",
    "#     sorted_pairs = sorted(zip(src_list, tgt_list), key=lambda x: -x[0].size(0))\n",
    "#     src_list, tgt_list = zip(*sorted_pairs)\n",
    "#     src_padded = pad_sequence(src_list, batch_first=True, padding_value=(vocab_en[PAD_TOKEN] if USE_TORCHTEXT else vocab_en.stoi[PAD_TOKEN]))\n",
    "#     tgt_padded = pad_sequence(tgt_list, batch_first=True, padding_value=(vocab_fr[PAD_TOKEN] if USE_TORCHTEXT else vocab_fr.stoi[PAD_TOKEN]))\n",
    "#     src_lengths = torch.tensor([s.size(0) for s in src_list], dtype=torch.long)\n",
    "#     tgt_lengths = torch.tensor([t.size(0) for t in tgt_list], dtype=torch.long)\n",
    "#     return src_padded, tgt_padded, src_lengths, tgt_lengths\n",
    "\n",
    "# # === Usage example ===\n",
    "# # load data\n",
    "# train_pairs = load_parallel(\"data/raw/train.en\", \"data/raw/train.fr\")\n",
    "# valid_pairs = load_parallel(\"data/raw/val.en\", \"data/raw/val.fr\")\n",
    "# test_pairs  = load_parallel(\"data/raw/test_2016_flickr.en\",  \"data/raw/test_2016_flickr.fr\")\n",
    "\n",
    "# # build vocabs (prefer torchtext if available)\n",
    "# if USE_TORCHTEXT:\n",
    "#     vocab_en = build_vocab_torchtext(train_pairs, lang=\"en\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ)\n",
    "#     vocab_fr = build_vocab_torchtext(train_pairs, lang=\"fr\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ)\n",
    "# else:\n",
    "#     vocab_en = build_vocab_counter(train_pairs, lang=\"en\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ)\n",
    "#     vocab_fr = build_vocab_counter(train_pairs, lang=\"fr\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ)\n",
    "\n",
    "# print(\"Vocab sizes -> EN:\", len(vocab_en) if USE_TORCHTEXT else len(vocab_en.itos),\n",
    "#       \" FR:\", len(vocab_fr) if USE_TORCHTEXT else len(vocab_fr.itos))\n",
    "\n",
    "# # create dataloaders\n",
    "# batch_size = 64\n",
    "# train_ds = ParallelDataset(train_pairs, vocab_en, vocab_fr)\n",
    "# valid_ds = ParallelDataset(valid_pairs, vocab_en, vocab_fr)\n",
    "# test_ds  = ParallelDataset(test_pairs,  vocab_en, vocab_fr)\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# print('DataLoaders ready. Example batch shapes:')\n",
    "# for src_batch, tgt_batch, src_lens, tgt_lens in train_loader:\n",
    "#     print(src_batch.shape, tgt_batch.shape, src_lens.shape, tgt_lens.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1b367fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processed_data import (build_vocab, load_parallel, ParallelDataset, make_collate_fn, encode_sentence_en, encode_sentence_fr, save_vocab, load_vocab, save_dataset_pytorch, load_dataset_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813844ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pairs = load_parallel('./data/raw/train.en', './data/raw/train.fr')\n",
    "# vocab_en = build_vocab(train_pairs, lang=\"en\")\n",
    "# vocab_fr = build_vocab(train_pairs, lang=\"fr\")\n",
    "\n",
    "# train_ds = ParallelDataset(train_pairs, vocab_en, vocab_fr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f7b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_vocab(vocab_en, './data/processed/vocab_en.pkl')\n",
    "# save_vocab(vocab_fr, './data/processed/vocab_fr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcbdf63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_pairs = load_parallel('./data/raw/val.en', './data/raw/val.fr')\n",
    "# test_pairs = load_parallel('./data/raw/test_2016_flickr.en', './data/raw/test_2016_flickr.fr')\n",
    "\n",
    "# valid_ds = ParallelDataset(valid_pairs, vocab_en, vocab_fr)\n",
    "# test_ds = ParallelDataset(test_pairs, vocab_en, vocab_fr)\n",
    "\n",
    "# save_dataset_pytorch(train_ds, './data/processed/train_dataset.pt')\n",
    "# save_dataset_pytorch(valid_ds, './data/processed/valid_dataset.pt')\n",
    "# save_dataset_pytorch(test_ds, './data/processed/test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a5e066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_en = load_vocab('./data/processed/vocab_en.pkl')\n",
    "vocab_fr = load_vocab('./data/processed/vocab_fr.pkl')\n",
    "\n",
    "train_ds = load_dataset_pytorch('./data/processed/train_dataset.pt')\n",
    "valid_ds = load_dataset_pytorch('./data/processed/valid_dataset.pt')\n",
    "test_ds = load_dataset_pytorch('./data/processed/test_dataset.pt')\n",
    "\n",
    "collate_fn = make_collate_fn(vocab_en, vocab_fr)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759fba6",
   "metadata": {},
   "source": [
    "# 3. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "091c1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_i = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.U_i = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.W_f = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.U_f = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.W_o = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.U_o = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.W_c = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.U_c = nn.Linear(hidden_size, hidden_size,  bias=True)\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        i_t = torch.sigmoid(self.W_i(x) + self.U_i(h_prev))\n",
    "        f_t = torch.sigmoid(self.W_f(x) + self.U_f(h_prev))\n",
    "        o_t = torch.sigmoid(self.W_o(x) + self.U_o(h_prev))\n",
    "        c_tilde_t = torch.tanh(self.W_c(x) + self.U_c(h_prev))\n",
    "        c_t = f_t * c_prev + i_t * c_tilde_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cells = nn.ModuleList([LSTMCell(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, h_0=None, c_0=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        if h_0 is None:\n",
    "            h_0 = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
    "        if c_0 is None:\n",
    "            c_0 = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
    "\n",
    "        h_n = []\n",
    "        c_n = []\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            for layer in range(self.num_layers):\n",
    "                h_prev = h_0[layer]\n",
    "                c_prev = c_0[layer]\n",
    "                h_t, c_t = self.cells[layer](x_t, h_prev, c_prev)\n",
    "                h_0[layer] = h_t\n",
    "                c_0[layer] = c_t\n",
    "                x_t = h_t\n",
    "            outputs.append(h_t.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        h_n = h_0\n",
    "        c_n = c_0\n",
    "\n",
    "        return outputs, (h_n, c_n)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        self.lstm = LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (h_n, c_n) = self.lstm(embedded)\n",
    "        return outputs, (h_n, c_n)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, num_layers=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        self.lstm = LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x, h_0, c_0):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (h_n, c_n) = self.lstm(embedded, h_0, c_0)\n",
    "        logits = self.fc(outputs)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return probs, (h_n, c_n)\n",
    "    \n",
    "    def forward_step(self, y_prev, h, c):\n",
    "        # y_prev: (batch,)\n",
    "        y_prev = y_prev.unsqueeze(1)         # (batch, 1)\n",
    "        embedded = self.embedding(y_prev)    # (batch, 1, embed)\n",
    "        \n",
    "        outputs, (h, c) = self.lstm(embedded, h, c)\n",
    "        logits = self.fc(outputs[:, -1, :])  # lấy token cuối\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        return probs, h, c\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, sos_id, eos_id):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size, device=src.device)\n",
    "\n",
    "        # encoder produce h, c\n",
    "        encoder_outputs, (h, c) = self.encoder(src)\n",
    "\n",
    "        # start token\n",
    "        y_prev = torch.full((batch_size,), self.sos_id, device=src.device)\n",
    "\n",
    "        for t in range(tgt_len):\n",
    "            probs, h, c = self.decoder.forward_step(y_prev, h, c)\n",
    "            outputs[:, t, :] = probs\n",
    "\n",
    "            # teacher forcing\n",
    "            use_tf = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            y_prev = tgt[:, t] if use_tf else probs.argmax(dim=-1)\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3581813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "encoder = Encoder(input_size=len(vocab_en), embed_size=256, hidden_size=512, num_layers=2)\n",
    "decoder = Decoder(output_size=len(vocab_fr), embed_size=256, hidden_size=512, num_layers=2)\n",
    "seq2seq_model = Seq2Seq(encoder, decoder, sos_id=vocab_fr['<sos>'], eos_id=vocab_fr['<eos>']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d163e",
   "metadata": {},
   "source": [
    "# 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "030d638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = vocab_fr['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = torch.optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=1, factor=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c3be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, num_epochs=20, teacher_forcing_ratio=0.5):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for src, tgt, src_lens, tgt_lens in train_loader:\n",
    "            \n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(src, tgt, teacher_forcing_ratio)   # (B, T, V)\n",
    "            outputs = outputs[:, :-1, :].reshape(-1, outputs.size(-1))\n",
    "            tgt_gold = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, tgt_gold)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for src, tgt, _, _ in valid_loader:\n",
    "                \n",
    "                src = src.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "\n",
    "                outputs = model(src, tgt, 0)\n",
    "                outputs = outputs[:, :-1, :].reshape(-1, outputs.size(-1))\n",
    "                tgt_gold = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "                loss = criterion(outputs, tgt_gold)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\"  → Saved best model\")\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 3:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc005746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train: 3924.4721 | Val: 138.0564\n",
      "  → Saved best model\n"
     ]
    }
   ],
   "source": [
    "train_model(seq2seq_model, train_loader, valid_loader, num_epochs=20, teacher_forcing_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c62dd6fa-4f6a-4bce-899a-849052339dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, vocab_en, vocab_fr, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # tokenize\n",
    "    tokens = sentence.lower().split()\n",
    "    ids = [vocab_en.stoi.get(tok, vocab_en.stoi[\"<unk>\"]) for tok in tokens]\n",
    "    src = torch.tensor(ids).unsqueeze(0).to(next(model.parameters()).device)\n",
    "\n",
    "    # encode\n",
    "    _, (h, c) = model.encoder(src)\n",
    "\n",
    "    # decode từng bước\n",
    "    y_prev = torch.tensor([vocab_fr.stoi[\"<sos>\"]], device=src.device)\n",
    "    result_ids = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        probs, h, c = model.decoder.forward_step(y_prev, h, c)\n",
    "        y_prev = probs.argmax(dim=-1)\n",
    "\n",
    "        token_id = y_prev.item()\n",
    "        if token_id == vocab_fr.stoi[\"<eos>\"]:\n",
    "            break\n",
    "        \n",
    "        result_ids.append(token_id)\n",
    "\n",
    "    # convert id -> word bằng vocab_fr.itos\n",
    "    words = [vocab_fr.itos[i] for i in result_ids]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409caed5-731f-4c88-b964-78dea7f05748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_bleu(test_data):\n",
    "    bleu_scores = []\n",
    "\n",
    "    for eng, fr in test_data:\n",
    "        pred = translate(eng)\n",
    "\n",
    "        reference = [fr.split()]   # gold\n",
    "        candidate = pred.split()   # prediction\n",
    "\n",
    "        bleu = sentence_bleu(reference, candidate)\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24c073d7-240f-46a3-8925-7ab978c8ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tensor(tensor_ids, vocab):\n",
    "    # vocab.itos là list: index -> token\n",
    "    tokens = []\n",
    "    for idx in tensor_ids.tolist():\n",
    "        if idx < len(vocab.itos):\n",
    "            tokens.append(vocab.itos[idx])\n",
    "    # bỏ pad/sos/eos\n",
    "    tokens = [t for t in tokens if t not in [\"<pad>\", \"<sos>\", \"<eos>\"]]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def show_examples(test_ds, model, vocab_en, vocab_fr, n=5):\n",
    "    for i in range(n):\n",
    "        src_tensor, tgt_tensor = test_ds[i]\n",
    "\n",
    "        eng = decode_tensor(src_tensor, vocab_en)\n",
    "        fr  = decode_tensor(tgt_tensor, vocab_fr)\n",
    "        pred = translate(eng, model, vocab_en, vocab_fr)\n",
    "\n",
    "        print(f\"\\n[Example {i+1}]\")\n",
    "        print(f\"EN:   {eng}\")\n",
    "        print(f\"FR:   {fr}\")\n",
    "        print(f\"PRED: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb09c44a-2ab1-4b88-8193-156dcb8e4e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Example 1]\n",
      "EN:   A man in an orange hat starring at something .\n",
      "FR:   Un homme avec un chapeau orange regardant quelque chose .\n",
      "PRED: Un homme un un un un un un un un un .\n",
      "\n",
      "[Example 2]\n",
      "EN:   A Boston Terrier is running on lush green grass in front of a white fence .\n",
      "FR:   Un terrier de Boston court sur l' herbe verdoyante devant une clôture blanche .\n",
      "PRED: Un homme homme un un un un un un un un un un un . .\n",
      "\n",
      "[Example 3]\n",
      "EN:   A girl in karate uniform breaking a stick with a front kick .\n",
      "FR:   Une fille en tenue de karaté brisant un bâton avec un coup de pied .\n",
      "PRED: Un homme homme un un un un un un un un un un . .\n",
      "\n",
      "[Example 4]\n",
      "EN:   Five people wearing winter jackets and helmets stand in the snow , with <unk> in the background .\n",
      "FR:   Cinq personnes avec des vestes d' hiver et des casques sont debout dans la neige , avec des <unk> en arrière-plan .\n",
      "PRED: Un homme homme un un un un un un un un un un un un un . .\n",
      "\n",
      "[Example 5]\n",
      "EN:   People are fixing the roof of a house .\n",
      "FR:   Des gens réparent le toit d' une maison .\n",
      "PRED: Un homme un un un un un un .\n"
     ]
    }
   ],
   "source": [
    "seq2seq_model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "seq2seq_model.eval()\n",
    "show_examples(test_ds, seq2seq_model, vocab_en, vocab_fr, n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d349ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_en.__dict__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
