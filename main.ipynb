{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d752adbd",
   "metadata": {},
   "source": [
    "# Seq2Seq Translation with LSTM  \n",
    "### English to French Machine Translation\n",
    "\n",
    "**Bao gồm:**\n",
    "- Chuẩn bị dữ liệu (raw → processed)\n",
    "- Tạo từ vựng (src + tgt)\n",
    "- Dataset + DataLoader\n",
    "- Encoder–Decoder LSTM\n",
    "- Training loop (teacher forcing)\n",
    "- Inference (translate function)\n",
    "- Evaluation (BLEU score)\n",
    "- 5 ví dụ dịch + phân tích lỗi\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b517346",
   "metadata": {},
   "source": [
    "### Import thư viện "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f66a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\ipykernel\\utils.py\", line 71, in preserve_context\n",
      "    return await f(*args, **kwargs)\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\MSI VN\\AppData\\Local\\Temp\\ipykernel_12924\\2202641564.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\torch\\__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e7f443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI VN\\.conda\\envs\\nlp310\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deaa634",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f370523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_parallel(en_path, fr_path):\n",
    "#     en_lines = open(en_path, 'r', encoding='utf-8').read().strip().split('\\n')\n",
    "#     fr_lines = open(fr_path, 'r', encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "#     assert len(en_lines) == len(fr_lines), \"Số dòng 2 file không khớp nhau!\"\n",
    "\n",
    "#     pairs = []\n",
    "\n",
    "#     for en, fr in zip(en_lines, fr_lines):\n",
    "#         pairs.append((en.lower(), fr.lower()))\n",
    "    \n",
    "#     return pairs\n",
    "\n",
    "# train_pairs = load_parallel('./data/raw/train.en', './data/raw/train.fr')\n",
    "# val_pairs = load_parallel('./data/raw/val.en', './data/raw/val.fr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b8d36",
   "metadata": {},
   "source": [
    "# 2. Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae03c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIL_TOKENS = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "\n",
    "# def tokenize(sentence):\n",
    "#     return sentence.strip().split()\n",
    "\n",
    "# def yield_tokens(pairs):\n",
    "#     for en, fr in pairs:\n",
    "#         for token in tokenize(en):\n",
    "#             yield token\n",
    "#         for token in tokenize(fr):\n",
    "#             yield token\n",
    "\n",
    "# def build_vocab(iterator):\n",
    "#     vocab = build_vocab_from_iterator(iterator, \n",
    "#                                     min_freq=2,\n",
    "#                                     specials=SPECIL_TOKENS,\n",
    "#                                     special_first=True,\n",
    "#                                     max_tokens=10000)\n",
    "#     vocab.set_default_index(vocab['<unk>'])\n",
    "#     return vocab\n",
    "\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a1cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Cell: Chuẩn bị dữ liệu (tokenize, build vocab, dataset, dataloader) ===\n",
    "# # Yêu cầu: raw files dạng data/raw/train.en, data/raw/train.fr, valid.en/valid.fr, test.en/test.fr\n",
    "# # Sử dụng spaCy tokenizer; vocab giới hạn 10000; thêm tokens: <unk>, <pad>, <sos>, <eos>.\n",
    "# # Nếu torchtext khả dụng -> dùng build_vocab_from_iterator; nếu không -> fallback bằng Counter.\n",
    "\n",
    "# import warnings\n",
    "# from collections import Counter\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# # --- spaCy tokenizer (bắt buộc đã cài en_core_web_sm và fr_core_news_sm) ---\n",
    "# import spacy\n",
    "# en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "# fr_nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# def tokenize_en(text):\n",
    "#     return [tok.text.lower() for tok in en_nlp(text)]\n",
    "\n",
    "# def tokenize_fr(text):\n",
    "#     return [tok.text.lower() for tok in fr_nlp(text)]\n",
    "\n",
    "# # --- Try to import torchtext build_vocab_from_iterator; fallback if not present ---\n",
    "# USE_TORCHTEXT = False\n",
    "# try:\n",
    "#     from torchtext.vocab import build_vocab_from_iterator\n",
    "#     USE_TORCHTEXT = True\n",
    "# except Exception as e:\n",
    "#     warnings.warn(\"torchtext unavailable or failed to load; falling back to Counter-based vocab builder.\\n\" + str(e))\n",
    "#     USE_TORCHTEXT = False\n",
    "\n",
    "# # --- Special tokens and options ---\n",
    "# PAD_TOKEN = \"<pad>\"\n",
    "# SOS_TOKEN = \"<sos>\"\n",
    "# EOS_TOKEN = \"<eos>\"\n",
    "# UNK_TOKEN = \"<unk>\"\n",
    "# SPECIALS = [UNK_TOKEN, PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "# MAX_VOCAB = 10000\n",
    "# MIN_FREQ = 1   # you can set to 2 to filter very rare tokens if desired\n",
    "\n",
    "# # --- Helpers to read parallel files (EN/FR) ---\n",
    "# def load_parallel(en_path, fr_path):\n",
    "#     with open(en_path, encoding=\"utf8\") as f:\n",
    "#         en_lines = [l.strip() for l in f if l.strip()]\n",
    "#     with open(fr_path, encoding=\"utf8\") as f:\n",
    "#         fr_lines = [l.strip() for l in f if l.strip()]\n",
    "#     assert len(en_lines) == len(fr_lines), f\"Mismatch lines: {len(en_lines)} != {len(fr_lines)}\"\n",
    "#     return list(zip(en_lines, fr_lines))\n",
    "\n",
    "# # Example usage:\n",
    "# # train_pairs = load_parallel(\"data/raw/train.en\", \"data/raw/train.fr\")\n",
    "\n",
    "# # --- Build vocab: two implementations ---\n",
    "# if USE_TORCHTEXT:\n",
    "#     # Using torchtext.build_vocab_from_iterator (preferred when available)\n",
    "#     def yield_tokens_from_pairs(pairs, lang=\"en\"):\n",
    "#         if lang == \"en\":\n",
    "#             for en, fr in pairs:\n",
    "#                 yield tokenize_en(en)\n",
    "#         else:\n",
    "#             for en, fr in pairs:\n",
    "#                 yield tokenize_fr(fr)\n",
    "\n",
    "#     def build_vocab_torchtext(pairs, lang=\"en\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ):\n",
    "#         iterator = yield_tokens_from_pairs(pairs, lang=lang)\n",
    "#         vocab = build_vocab_from_iterator(iterator,\n",
    "#                                           specials=SPECIALS,\n",
    "#                                           special_first=True,\n",
    "#                                           max_tokens=max_tokens)\n",
    "#         vocab.set_default_index(vocab[UNK_TOKEN])\n",
    "#         return vocab\n",
    "\n",
    "# else:\n",
    "#     # Fallback: use Counter and build dict\n",
    "#     def build_vocab_counter(pairs, lang=\"en\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ):\n",
    "#         counter = Counter()\n",
    "#         if lang == \"en\":\n",
    "#             for en, fr in pairs:\n",
    "#                 counter.update(tokenize_en(en))\n",
    "#         else:\n",
    "#             for en, fr in pairs:\n",
    "#                 counter.update(tokenize_fr(fr))\n",
    "#         # keep most common up to max_tokens - len(SPECIALS)\n",
    "#         most_common = [w for w, c in counter.most_common(max_tokens - len(SPECIALS)) if c >= min_freq]\n",
    "#         vocab_list = SPECIALS + most_common\n",
    "#         stoi = {w: i for i, w in enumerate(vocab_list)}\n",
    "#         # helper object to mimic torchtext vocab API minimally\n",
    "#         class VocabLike:\n",
    "#             def __init__(self, stoi, itos):\n",
    "#                 self.stoi = stoi\n",
    "#                 self.itos = itos\n",
    "#             def __len__(self):\n",
    "#                 return len(self.itos)\n",
    "#             def __getitem__(self, token):\n",
    "#                 return self.stoi.get(token, self.stoi[UNK_TOKEN])\n",
    "#             def get_itos(self):\n",
    "#                 return self.itos\n",
    "#             def get_stoi(self):\n",
    "#                 return self.stoi\n",
    "#             def set_default_index(self, idx):\n",
    "#                 pass\n",
    "#         itos = vocab_list\n",
    "#         return VocabLike(stoi, itos)\n",
    "\n",
    "# # --- Encode / numericalize helpers (works for both vocab types) ---\n",
    "# def tokens_to_ids(tokens, vocab, specials_map=None):\n",
    "#     # vocab could be torchtext.Vocab or our VocabLike\n",
    "#     ids = []\n",
    "#     if USE_TORCHTEXT:\n",
    "#         for t in tokens:\n",
    "#             ids.append(vocab[t])\n",
    "#     else:\n",
    "#         for t in tokens:\n",
    "#             ids.append(vocab.stoi.get(t, vocab.stoi[UNK_TOKEN]))\n",
    "#     return ids\n",
    "\n",
    "# def encode_sentence_en(text, vocab):\n",
    "#     toks = tokenize_en(text)\n",
    "#     ids = [ (vocab[SOS_TOKEN] if USE_TORCHTEXT else vocab.stoi[SOS_TOKEN]) ]\n",
    "#     ids += tokens_to_ids(toks, vocab)\n",
    "#     ids += [ (vocab[EOS_TOKEN] if USE_TORCHTEXT else vocab.stoi[EOS_TOKEN]) ]\n",
    "#     return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# def encode_sentence_fr(text, vocab):\n",
    "#     toks = tokenize_fr(text)\n",
    "#     ids = [ (vocab[SOS_TOKEN] if USE_TORCHTEXT else vocab.stoi[SOS_TOKEN]) ]\n",
    "#     ids += tokens_to_ids(toks, vocab)\n",
    "#     ids += [ (vocab[EOS_TOKEN] if USE_TORCHTEXT else vocab.stoi[EOS_TOKEN]) ]\n",
    "#     return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# # --- Dataset + collate_fn (pad + sort by length descending + return lengths) ---\n",
    "# class ParallelDataset(Dataset):\n",
    "#     def __init__(self, pairs, src_vocab, tgt_vocab):\n",
    "#         self.pairs = pairs\n",
    "#         self.src_vocab = src_vocab\n",
    "#         self.tgt_vocab = tgt_vocab\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.pairs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         en, fr = self.pairs[idx]\n",
    "#         src_ids = encode_sentence_en(en, self.src_vocab)\n",
    "#         tgt_ids = encode_sentence_fr(fr, self.tgt_vocab)\n",
    "#         return src_ids, tgt_ids\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     # batch: list of (src_tensor, tgt_tensor)\n",
    "#     src_list, tgt_list = zip(*batch)\n",
    "#     # sort by src length desc (required for pack_padded_sequence)\n",
    "#     sorted_pairs = sorted(zip(src_list, tgt_list), key=lambda x: -x[0].size(0))\n",
    "#     src_list, tgt_list = zip(*sorted_pairs)\n",
    "#     src_padded = pad_sequence(src_list, batch_first=True, padding_value=(vocab_en[PAD_TOKEN] if USE_TORCHTEXT else vocab_en.stoi[PAD_TOKEN]))\n",
    "#     tgt_padded = pad_sequence(tgt_list, batch_first=True, padding_value=(vocab_fr[PAD_TOKEN] if USE_TORCHTEXT else vocab_fr.stoi[PAD_TOKEN]))\n",
    "#     src_lengths = torch.tensor([s.size(0) for s in src_list], dtype=torch.long)\n",
    "#     tgt_lengths = torch.tensor([t.size(0) for t in tgt_list], dtype=torch.long)\n",
    "#     return src_padded, tgt_padded, src_lengths, tgt_lengths\n",
    "\n",
    "# # === Usage example ===\n",
    "# # load data\n",
    "# train_pairs = load_parallel(\"data/raw/train.en\", \"data/raw/train.fr\")\n",
    "# valid_pairs = load_parallel(\"data/raw/val.en\", \"data/raw/val.fr\")\n",
    "# test_pairs  = load_parallel(\"data/raw/test_2016_flickr.en\",  \"data/raw/test_2016_flickr.fr\")\n",
    "\n",
    "# # build vocabs (prefer torchtext if available)\n",
    "# if USE_TORCHTEXT:\n",
    "#     vocab_en = build_vocab_torchtext(train_pairs, lang=\"en\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ)\n",
    "#     vocab_fr = build_vocab_torchtext(train_pairs, lang=\"fr\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ)\n",
    "# else:\n",
    "#     vocab_en = build_vocab_counter(train_pairs, lang=\"en\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ)\n",
    "#     vocab_fr = build_vocab_counter(train_pairs, lang=\"fr\", max_tokens=MAX_VOCAB, min_freq=MIN_FREQ)\n",
    "\n",
    "# print(\"Vocab sizes -> EN:\", len(vocab_en) if USE_TORCHTEXT else len(vocab_en.itos),\n",
    "#       \" FR:\", len(vocab_fr) if USE_TORCHTEXT else len(vocab_fr.itos))\n",
    "\n",
    "# # create dataloaders\n",
    "# batch_size = 64\n",
    "# train_ds = ParallelDataset(train_pairs, vocab_en, vocab_fr)\n",
    "# valid_ds = ParallelDataset(valid_pairs, vocab_en, vocab_fr)\n",
    "# test_ds  = ParallelDataset(test_pairs,  vocab_en, vocab_fr)\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# print('DataLoaders ready. Example batch shapes:')\n",
    "# for src_batch, tgt_batch, src_lens, tgt_lens in train_loader:\n",
    "#     print(src_batch.shape, tgt_batch.shape, src_lens.shape, tgt_lens.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b367fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processed_data import (build_vocab, load_parallel, ParallelDataset, collate_fn, encode_sentence_en, encode_sentence_fr, save_vocab, load_vocab, save_dataset_pytorch, load_dataset_pytorch, create_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "813844ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = load_parallel('./data/raw/train.en', './data/raw/train.fr')\n",
    "vocab_en = build_vocab(train_pairs, lang=\"en\")\n",
    "vocab_fr = build_vocab(train_pairs, lang=\"fr\")\n",
    "\n",
    "train_ds = ParallelDataset(train_pairs, vocab_en, vocab_fr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f7b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(vocab_en, './data/processed/vocab_en.pkl')\n",
    "save_vocab(vocab_fr, './data/processed/vocab_fr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcbdf63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pairs = load_parallel('./data/raw/val.en', './data/raw/val.fr')\n",
    "test_pairs = load_parallel('./data/raw/test_2016_flickr.en', './data/raw/test_2016_flickr.fr')\n",
    "\n",
    "valid_ds = ParallelDataset(valid_pairs, vocab_en, vocab_fr)\n",
    "test_ds = ParallelDataset(test_pairs, vocab_en, vocab_fr)\n",
    "\n",
    "save_dataset_pytorch(train_ds, './data/processed/train_dataset.pt')\n",
    "save_dataset_pytorch(valid_ds, './data/processed/valid_dataset.pt')\n",
    "save_dataset_pytorch(test_ds, './data/processed/test_dataset.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
