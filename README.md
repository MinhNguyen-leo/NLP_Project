# NLP_Project
Neural Machine Translation (EN â†’ FR)
ğŸš€ Seq2Seq â€¢ Bahdanau Attention â€¢ Beam Search â€¢ BPE Tokenization
<p align="center"> <img src="https://img.shields.io/badge/PyTorch-2.0+-red?logo=pytorch" /> <img src="https://img.shields.io/badge/Python-3.9+-blue?logo=python" /> <img src="https://img.shields.io/badge/License-MIT-green" /> <img src="https://img.shields.io/badge/Model-Baseline%20%7C%20Attention-orange" /> </p> <p align="center"> <img src="./images/LSTM.png.png" width="750"> </p>


# ğŸ“‘ Table of Contents
- [ğŸ“˜ 1. Giá»›i thiá»‡u](#-1-giá»›i-thiá»‡u)
- [ğŸ“‚ 2. Cáº¥u trÃºc dá»± Ã¡n](#-2-cáº¥u-trÃºc-dá»±-Ã¡n)
- [ğŸ“Š 3. Dá»¯ liá»‡u vÃ  xá»­ lÃ½ dá»¯ liá»‡u](#-3-dá»¯-liá»‡u-vÃ -xá»­-lÃ½-dá»¯-liá»‡u)
- [ğŸ§  4. Kiáº¿n trÃºc mÃ´ hÃ¬nh](#-4-kiáº¿n-trÃºc-mÃ´-hÃ¬nh)
- [ğŸ‹ï¸ 5. Huáº¥n luyá»‡n mÃ´ hÃ¬nh](#-5-huáº¥n-luyá»‡n-mÃ´-hÃ¬nh)
- [ğŸ“ˆ 6. Káº¿t quáº£](#-6-káº¿t-quáº£)
- [ğŸ’¬ 7. VÃ­ dá»¥ dá»‹ch](#-7-vÃ­-dá»¥-dá»‹ch)
- [ğŸ“¦ 8. Táº£i mÃ´ hÃ¬nh ](#-8-táº£i-mÃ´-hÃ¬nh-pretrained)
- [ğŸ“ 9. Káº¿t luáº­n](#-10-káº¿t-luáº­n)

---

# ğŸ“˜ 1. Giá»›i thiá»‡u

Dá»± Ã¡n xÃ¢y dá»±ng há»‡ thá»‘ng **dá»‹ch tá»± Ä‘á»™ng Neural Machine Translation (NMT)** tá»« **Tiáº¿ng Anh â†’ Tiáº¿ng PhÃ¡p** gá»“m:

- **MÃ´ hÃ¬nh gá»‘c (Baseline):** Seq2Seq LSTM  
- **MÃ´ hÃ¬nh má»Ÿ rá»™ng:** Seq2Seq + **Bahdanau Attention**  
- **Beam Search** má»©c 5 Ä‘á»ƒ tÄƒng cháº¥t lÆ°á»£ng sinh cÃ¢u  
- **BPE Tokenization** Ä‘á»ƒ háº¡n cháº¿ OOV vÃ  tá»‘i Æ°u biá»ƒu diá»…n tá»«

Má»¥c tiÃªu:  
âœ” So sÃ¡nh cháº¥t lÆ°á»£ng dá»‹ch 2 mÃ´ hÃ¬nh  
âœ” TrÃ¬nh bÃ y pipeline xá»­ lÃ½ dá»¯ liá»‡u â€“ huáº¥n luyá»‡n â€“ Ä‘Ã¡nh giÃ¡  
âœ” BÃ¡o cÃ¡o BLEU score vÃ  phÃ¢n tÃ­ch lá»—i  

---

# ğŸ“‚ 2. Cáº¥u trÃºc dá»± Ã¡n
```bash

ğŸ“¦ NLP_Project
â”‚â”€â”€ data/
â”‚ â”œâ”€â”€ raw/                         # chá»©a dá»¯ liá»‡u raw
â”‚ â”œâ”€â”€ proceesed/                   # chá»©a dá»¯ liá»‡u Ä‘Ã£ qua xá»­ 
â”‚â”€â”€ best_model.pth                 # best model baseline
â”‚â”€â”€ best_attn.pth                  # best model attention
â”‚â”€â”€ processed_data.py              # Script tiá»n xá»­ lÃ½ dá»¯ liá»‡u 
â”‚â”€â”€ processed_data_bpe.py          # Script tiá»n xá»­ lÃ½ BPE
â”‚â”€â”€ main.ipynb
â””â”€â”€ README.md

```

---

# ğŸ“Š 3. Dá»¯ liá»‡u vÃ  xá»­ lÃ½ dá»¯ liá»‡u

### âœ” Dataset
Sá»­ dá»¥ng táº­p dá»¯ liá»‡u **Tatoeba / ManyThings ENâ€“FR**, gá»“m:
- 29k cÃ¢u train  
- 1k cÃ¢u validation  
- 1k cÃ¢u test  

### âœ” Baseline preprocessing (spaCy)
- TÃ¡ch tá»« theo word-level  
- Token `<unk>` xuáº¥t hiá»‡n nhiá»u  
- Dá»… gÃ¢y máº¥t thÃ´ng tin á»Ÿ tá»« hiáº¿m

### âœ” Extended preprocessing (BPE â€“ SentencePiece)
- Vocab 4000 subwords  
- Giáº£m tá»« chÆ°a tháº¥y (OOV)  
- Cáº£i thiá»‡n phÃ¢n rÃ£ tá»« â†’ mÃ´ hÃ¬nh dá»… há»c hÆ¡n  

---

# ğŸ§  4. Kiáº¿n trÃºc mÃ´ hÃ¬nh

## **4.1. Baseline â€“ Seq2Seq LSTM**
<p align="center">
  <img src="images/baseline_arch.png" width="650">
</p>

- Encoder: 2-layer LSTM  
- Decoder: 2-layer LSTM  
- KhÃ´ng cÃ³ attention  
- ThÃ´ng tin cÃ¢u dÃ i bá»‹ â€œquÃªnâ€ â†’ dá»‹ch kÃ©m á»Ÿ cÃ¢u dÃ i  

---

## **4.2. MÃ´ hÃ¬nh má»Ÿ rá»™ng â€“ Bahdanau Attention**
<p align="center">
  <img src="images/attention_arch.png" width="650">
</p>

Cáº£i thiá»‡n:
- Giá»¯ Ä‘Æ°á»£c ngá»¯ cáº£nh tá»‘t hÆ¡n  
- Giáº£m lá»—i láº·p tá»«  
- Táº­p trung vÃ o token quan trá»ng trong tá»«ng bÆ°á»›c sinh  

---

## **4.3. Beam Search (size = 5)**
Giá»¯ nhiá»u giáº£ thuyáº¿t cÃ¢u cÃ¹ng lÃºc

TrÃ¡nh greedy decoding (thÆ°á»ng bá»‹ quÃ¡ tham Ä‘á»‹a phÆ°Æ¡ng)

length_penalty = 0.7 Ä‘á»ƒ giáº£m bias cÃ¢u ngáº¯n

---

# ğŸ‹ï¸ 5. Huáº¥n luyá»‡n mÃ´ hÃ¬nh

## **Baseline**
| ThÃ nh pháº§n | Cáº¥u hÃ¬nh |
|-----------|----------|
| Optimizer | Adam (lr=0.001) |
| Loss | CrossEntropy (ignore pad_id) |
| Epoch | 20 |
| Teacher Forcing | 0.5 |
| Batch size | 64 |
| Scheduler | ReduceLROnPlateau |
| Early stopping | patience = 3 |

---

## **Attention Model**
| ThÃ nh pháº§n | Cáº¥u hÃ¬nh |
|-----------|----------|
| Hidden size | **512** |
| Embedding | **320** |
| Dropout | 0.3 |
| Teacher forcing | 0.7 â†’ 0.1 |
| Epoch | 47 (early-stopped) |
| Optimizer | Adam (lr=3e-4) |
| Scheduler | ReduceLROnPlateau |
| Beam size | 5 |

---

# ğŸ“ˆ 6. Káº¿t quáº£

## **BLEU Score**
| MÃ´ hÃ¬nh | BLEU |
|--------|-------|
| **Seq2Seq Baseline** | **0.3832** |
| **Seq2Seq + Attention** | **0.4432** |

ğŸ‘‰ Attention **tÄƒng 23%** BLEU so vá»›i baseline.  
ğŸ‘‰ Giáº£m rÃµ rá»‡t lá»—i láº·p tá»«, máº¥t thÃ´ng tin, dá»‹ch sai ngá»¯ nghÄ©a.

---

# ğŸ’¬ 7. VÃ­ dá»¥ dá»‹ch

### **Baseline**
| EN | REF | PRED |
|----|-----|-------|
| A man in an orange hatâ€¦ | un hommeâ€¦ | un homme avec un orange orangeâ€¦ |
| A Boston Terrierâ€¦ | un terrierâ€¦ | un gardien de hockeyâ€¦ |

---

### **Attention Model**
| EN | REF | PRED |
|----|-----|-------|
| a man in an orange hatâ€¦ | un hommeâ€¦ | un homme avec un casquette orangeâ€¦ |
| a boston terrierâ€¦ | un terrierâ€¦ | un joueur de bk courtâ€¦ |

---

# ğŸ“¦ 8. Táº£i mÃ´ hÃ¬nh 

â¡ **GitHub Releases:**  
https://github.com/MinhNguyen-leo/NLP_Project/releases/tag/nlp

### Táº£i 2 attention model:

```python

import requests

def download_from_github(url, output_path):
    print(f"Downloading from {url} ...")
    r = requests.get(url)
    if r.status_code == 200:
        with open(output_path, "wb") as f:
            f.write(r.content)
        print(f"Saved: {output_path}")
    else:
        print("Failed to download:", r.status_code)

# Load last model vÃ  best model tá»« Github Releases
baseline_url = "https://github.com/MinhNguyen-leo/NLP_Project/releases/download/nlp/last_attn.pth"
attn_url     = "https://github.com/MinhNguyen-leo/NLP_Project/releases/download/nlp/best_attn.pth"

download_from_github(baseline_url, "last_attn.pth")
download_from_github(attn_url,     "best_attn.pth")
```

# ğŸ“ 9. Káº¿t luáº­n

* Baseline Seq2Seq háº¡n cháº¿: dá»… láº·p tá»«, máº¥t ngá»¯ cáº£nh, dá»‹ch sai danh tá»« riÃªng.

* Attention cáº£i thiá»‡n máº¡nh: giá»¯ Ä‘Æ°á»£c thÃ´ng tin toÃ n cÃ¢u, táº­p trung vÃ o token quan trá»ng.

* Beam Search giÃºp cÃ¢u dá»‹ch mÆ°á»£t & tá»± nhiÃªn hÆ¡n.

* BLEU tÄƒng tá»« 0.38 â†’ 0.47.


